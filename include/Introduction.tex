\chapter{Introduction}

\section{Aim and background}
In the era of big data, mining graphs for information is getting increasingly more popular. An example is to retrieve how central a given node is, which can be measured by node reachability. Node centrality has plenty of real world applications. Consider a graph of companies with edges representing some relations (for example whether they collaborate or not). Node centrality can determine which companies are well established. 

The neighborhood function (NF) is used for determining node reachability in graphs. For each node $v$ in a graph, the NF is used to determine how many other nodes $v$ can reach in a limited number of steps. Formally speaking, given a graph $G = (V,E)$, $NF(G,h) = \{|\{v : v \in V, dist(u,v) \leq h \}| : u \in V\}$, where $h$ is the specified maximum number of steps. Due to the dependence on the value $h$, a variety of graph properties can be expressed by the NF. For example, the diameter of a graph can be expressed as the smallest number $d$ where $NF(G,d) = NF(G,\infty)$. By setting $h$ to a fixed value $h < d$ the fraction of a graph that a node can reach in $h$ steps can be calculated.

The NF can be calculated exactly in either $O(n^{2.38})$ operations and $O(n^2)$ space units or $O(nm)$ operations and $O(m + n)$ space units, where $n$ is the number of nodes and $m$ is the number of edges \cite{Palmer01}. However, for very large graphs these polynomial bounds become infeasible. Therefore, recent research has focused on the development of approximation algorithms to calculate the NF for large graphs \cite{Palmer01,hyperanf,anf}. HyperANF is a state-of-the-art algorithm, created by P.Boldi et al., that provides an approximation of the NF \cite{hyperanf}. HyperANF uses HyperLogLog counters to approximate the number of nodes a given node can reach. HyperLogLog counters are statistical counters that only require $O( \log\log n)$ bits to approximate the cardinality of a multiset up to size $n$ \cite{hyperloglog}. In HyperANF, each node is given a constant number of counters and hence requires $O(n \log\log n)$ space. The time complexity for HyperANF is not known but it is an extension to ANF which runs in $O((n+m)h)$ operations. A similar time complexity is expected of HyperANF. The authors of \cite{hyperanf} provide benchmarks on public data-sets that empirically show that HyperANF is much faster than its predecessors, including ANF. Due to HyperANF's low space complexity, it works well on graphs with billions of nodes. Currently, the algorithm only support  static graphs. This paper aims to extend the existing algorithm to supports dynamic graphs. We call our algorithm Dynamic Approximate Neighborhood Function (DANF).

NF for dynamic graphs can be an efficient tool to continuously gather information from very large graphs and see changes in centrality over time. In the above example of companies, the NF can be used to calculate which companies are most central at the moment. If the graph is updated in real time, it is interesting to see how the graph evolves over time as this can give insight into the companies that are growing. Seeing this information might be an advantage for a stock trader, for example. The NF can be used repeatedly to monitor changes as well, but depending on the size of the graph, it can take hours to recalculate. The stock market might have already been closed for the day when the recalculation is finished. Instead, a dynamic NF can provide a continuously updated graph throughout the day with substantially less effort. Then the trader can get updates anytime throughout the day with a very short delay.

\section{Problem formulation}
Let $G_i$ be a directed graph $G_i = (V_i,E_i)$ at time $i = 1,2,3,...$.
Let $A_i$ be the set of added nodes in $G_i$ and $D_i$ the set of deleted nodes. 
At each time step $i > 1$ $G_i = (G_{i-1} \backslash D_{i-1}) \cup A_{i-1}$ and $G_1$ is the original graph. Given the NF for $G_1$, our aim is to continuously update the NF for $G_{i>1}$ without performing a complete recalculation of $NF(h,G_i)$. Additionally, the solution should work for very large graphs. Hence, it must be scalable. 

The algorithm will be used by Meltwater to find important entities in their data. A graph will be initially built from the data collected by Meltwater. The data comes from sources such as news papers and social networks. Whenever new information is collected, the graph should be updated to maintain relevant information. There are roughly two million new documents received each day and the number of existing documents are in the scale of billions. To be able to handle these magnitudes, the time and space complexity of the proposed algorithm are very limited.

The data stream from Meltwater have some specific properties that should be accounted for in the algorithm: The items are rarely removed from the data. Hence, insertions will be much more common than deletions. The original graph is very large, and the data-stream is relatively small compared to the original graph. 

\section{Limitations}
\subsection{Entity disambiguating}
A notorious problem in graph construction is how to disambiguate the nodes. For example, assume that there is a graph consisting of nodes representing individual identities. Suppose that there exists a node with the label Anders Svensson. Next, an article regarding a person with a similar name is added to the graph. Entity disambiguation is required to determine if the article refers to the currently existing Anders or if a new node should be created. Disambiguating entities is important in the construction of the graph, as false results are likely to occur otherwise. Meltwater has an NLP-department which already disambiguates entities with a good performance. Therefore, it is assumed that the entity disambiguation, already in place by Meltwater, is correct. 

\section{Mathematical preliminaries}

\subsection{Graph transpose}
The transpose of a graph is a graph with all of its edges flipped. A function $T$ calculates the transpose of a graph $G$ in the following manner: $T((V,E)) = (V,\{(u,v) : (v,u) \in E\})$.

\subsection{Maximal matching}
Given a graph $G = (V,E)$, a matching is a subset $M \subseteq E$ such that all nodes are incident to at most one edge in $M$. A maximal matching is a matching such that if another edge is added to it, it is no longer a matching. 

\subsection{Approximation algorithms}
Approximation algorithms are useful to calculate a non-optimal answer quickly where the optimal solution would take too long to calculate. A \textit{C}-approximation means that the solution is in worst case \textit{C} times the size of the optimal solution. For example, if you calculate the minimal vertex cover with a 2-approximation, the solution can be at most twice as large as the optimal solution. If you want to calculate the maximal matching with a $\frac{1}{2}$-approximation the size of the solution can be no less than half of the optimal solution.

\subsection{Minimum vertex cover}
Given an undirected graph $G = (V,E)$, a vertex cover is a subset $S \subseteq V$ such that for all edges $e = (u,v) \in E$, $ u \in S \vee v \in S$. A minimum vertex cover is a set $S$ of minimum size. The problem of finding a minimum vertex cover is NP-complete \cite{Kar72}.