\chapter{Introduction}

\section{Aim and background}
In the era of big data, mining graphs for information is getting increasingly more popular. An example is to retrieve how central a given node is, which can be measured by node reachability. Node centrality has plenty of real world applications. For example, consider a graph of companies with edges representing some relations (for example whether they collaborate or not). Node centrality can determine which companies are well established. 

The neighborhood function (NF) is used for determining node reachability in graphs. For each node in a graph the NF calculates how many other nodes it can reach in a limited number of steps. Formally speaking, given a graph $G = (V,E)$ NF calculates $\forall u \in V,$ $N(u,h) = |\{v : v \in V, dist(u,v) \leq h \}|$, where $h$ is the specified maximum number of steps. Due to the dependence on the value $h$, a variety of graph properties can be expressed by the NF. For example, the diameter of a graph can be expressed as the smallest number $d$ where $ \forall u \in V$, $N(u,d) = N(u,\infty)$. By setting h to a fixed value $h < d$ the fraction of a graph that a node can reach in h steps can be calculated.

The NF can be calculated exactly in either $O(n^{2.38})$ operations and $O(n^2)$ space units or $O(nm)$ operations and $O(m + n)$ space units, where $n$ is the number of nodes and $m$ is the number of edges \cite{Palmer01}. However, for very large graphs these polynomial bounds become infeasible. Therefore, recent research has focused on the development of approximation algorithms to calculate the NF for large graphs \cite{Palmer01,hyperanf,anf}. HyperANF is a state-of-the-art algorithm, created by P.Boldi et al., that provides an approximation of the NF \cite{hyperanf}. HyperANF uses HyperLogLog counters to approximate the number of nodes a given node can reach. HyperLogLog counters are statistical counters that require only $O( \log\log n)$ bits to approximate the cardinality of a multiset up to size $n$ \cite{hyperloglog}. In HyperANF, each node is given a constant number of counters and hence requires $O(n \log\log n)$ space. The time complexity for HyperANF is not known but it is an extension to ANF which runs in $O((n+m)h)$ operations. Hence a similar time complexity is expected of HyperANF. The authors of \cite{hyperanf} provide benchmarks on public data-sets that empirically show that HyperANF is much faster than its predecessors, including ANF. Due to HyperANFs low space complexity, it works well on graphs with billions of nodes. Currently, the algorithm only support  static graphs. This paper aims to extend the existing algorithm to support dynamic graphs. We call our algorithm Dynamic Approximate Neighborhood Function (DANF).

NF for dynamic graphs can be an efficient tool to continuously gather information from very large graphs and see changes in centrality over time. In the above example of companies, the NF can be used to calculate which companies are most central at the moment. If the graph is updated in real time, it is interesting to see how the graph evolves over time as this can give insight into which companies that are growing. A stock trader might have an advantage of seeing this information. The NF can be used repeatedly to monitor changes as well, but depending on how large the graph is it can take hours to recalculate. The stock market might have already closed for the day after the recalculation is finished. Instead, a dynamic NF can provide a continuously updated graph throughout the day. The trader can then get updates anytime throughout the day without any delay.

\section{Problem formulation}
The formalized problem is as follows: 
Let $G_i$ be the graph $G_i = (V_i,E_i)$ at time $i = 1,2,3,...$.
Let $A_i$ be the set of added nodes in $G_i$ and $D_i$ the set of deleted nodes. 
At each time step $i > 1$ $G_i = (G_{i-1} \backslash D_{i-1}) \cup A_{i-1}$ and $G_1$ is the original graph. Given the NF $N(u, h) = |\{v : v \in V, dist(u,v) \leq h \}|$ for all $u \in V$ in $G_1$, continuously update the NF for $G_{i>1}$ without performing a complete recalculation of $\forall u \in V_i$ $N(u, h)$ . Additionally, the solution should work for very large graphs, hence the solution must be scalable. 

The algorithm will be used by Meltwater to find important entities in their data. A graph will initially be built from the data collected by Meltwater. The data comes from sources such as news papers and social networks. Whenever information is collected the graph should be updated to continuously contain relevant information. There are roughly two million new documents received each day and the number of existing documents are in the scale of billions. To be able to handle these magnitudes, the time and space complexity of the proposed algorithm are very limited.

The data stream from Meltwater have some specific properties that can be accounted for in the algorithm. It is rarely the case that they remove items and connections from the data, hence insertions will be much more common than deletions. The original graph is very large, and the data-stream is relatively small compared to the original graph. 

\section{Limitations}
\subsection{Entity disambiguating}
A problem that occurs when constructing graphs is how to disambiguate the nodes. For example, assume there is a graph consisting of nodes that represents individual identities. Suppose that there exists a node with the label Anders Svensson. Next, an article regarding a person with similar name is added to the graph. Entity disambiguation is required to determine if the article refers to the currently existing Anders or if a new node should be created. Disambiguating entities is important in the construction of the graph, as false results are likely to occur otherwise. Meltwater has an NLP-department which already disambiguates entities with a good performance. Therefore, it is assumed that the entity disambiguation, already in place by Meltwater, is correct. 