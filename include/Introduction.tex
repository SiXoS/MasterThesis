\chapter{Introduction}

\section{Aim and background}
In the era of big data, mining graphs for information is getting increasingly more popular. An example of what data can be retrieved from a graph is how central a node is. Node centrality has plenty of real world applications. For example, think of a graph of companies with some relations (for example if they collaborate on projects). Node centrality can determine which companies are the most central and hence the most influential. 

The neighborhood function (NF) is used for determining node reachability in graphs which can be used as a measure of a node's centrality. For each node in a graph the NF calculates how many other nodes it can reach in a limited number of steps. Formally speaking, given a graph $G = (V,E)$ NF calculates $\forall u \in V,$ $N(u,h) = |\{v : v \in V, dist(u,v) \leq h \}|$, where $h$ is the specified maximum number of steps. Due to the dependence on the value $h$, a variety of graph properties can be expressed by the NF. For example, the diameter of a graph can be expressed as the smallest number $d$ where $ \forall u \in V$, $N(u,d) = N(u,\infty)$. By setting h to a fixed value $h < d$ the fraction of a graph that a node can reach in h steps can be calculated.

The neighborhood function can be calculated exactly in either $O(n^{2.38})$ operations and $O(n^2)$ space units or $O(nm)$ operations and $O(m + n)$ space units, where $n$ is the number of nodes and $m$ is the number of edges \cite{Palmer01}. However, for very large graphs these polynomial bounds become infeasible. Therefore, recent research have focused on the development of approximation algorithms to calculate the neighborhood function \cite{hyperanf,Palmer01,anf}. 

HyperANF is a state-of-the-art algorithm, created by P.Boldi et al., that provides an approximation for the neighborhood function \cite{hyperanf}. HyperANF uses HyperLogLog counters to approximate the number of nodes a given node can reach. HyperLogLog counters are statistical counters that require only $O( \log\log n)$ bits to approximate the cardinality of a multiset up to size $n$ \cite{hyperloglog}. In HyperANF each node is given a constant number of counters, and hence requires $O(n \log\log n)$ space. However, HyperANF does not have any stated time complexity, but it is an extension to ANF, which runs in $O((n+m)h)$, where h is the number of steps. The authors provide benchmarks on public data-sets that empirically show that it is much faster that its predecessors, including ANF. Due to HyperANFs low space complexity, it works well on graphs with billions of edges. Currently the algorithm supports only static graphs. This paper aims to extend the existing algorithm to support dynamic graphs. The algorithm developed is called Dynamic Approximate Neighborhood Function (DANF).

NF for dynamic graphs can be an efficient tool to continuously gather information from very large graphs and see changes in influence over time. In the companies example above, the NF can be used to calculate which companies are most influential at the moment. If the graph is updated in real time, it would be interesting to see how the graph evolve over time as this can give insight into which companies are gaining more and more influence. A stock trader might have a huge advantage of seeing this information. The NF can be used repeatedly to monitor changes as well, but depending on how large the graph is it can take hours to recalculate. The stock market might have already closed for the day after the recalculation is finished. Instead, a dynamic NF can provide a continuously updated graph throughout the day. The trader can then get updates anytime throughout the day without any delay.

\section{Problem formulation}
The formalized problem is as follows: 
Let $G_i$ be the graph $G_i = (V_i,E_i)$ at time $i = 1,2,3,...$.
Let $A_i$ be the set of added nodes in $G_i$ and $D_i$ the set of deleted nodes. 
At each time step $i > 1$ $G_i = (G_{i-1} \backslash D_{i-1}) \cup A_{i-1}$ and $G_1$ is the original graph. Given the neighborhood function $N(u, h) = |\{v : v \in V, dist(u,v) \leq h \}|$ for all $u \in V$ in $G_1$, continuously update the neighborhood function for $G_{i>1}$ without performing a complete recalculation of $\forall u \in V_i$ $N(u, h)$ . Additionally, the solution should work for very large graphs, hence the solution must be scalable. 

The algorithm will be used by Meltwater to find important entities in their data. A graph will initially be built from the data collected by Meltwater. The data comes from sources such as news papers and social networks. Whenever information is collected the graph should be updated to continuously contain relevant information. There are roughly two million new documents received each day and the number of existing documents are in the scale of billions. To be able to handle these magnitudes, the time and space complexity of the proposed algorithm are very limited.

The data stream from Meltwater have some specific properties that can be accounted for in the algorithm. It is rarely the case that they remove items and connections from the data, hence insertions will be much more common than deletions. The original graph is very large, and the data-stream is relatively small compared to the original graph. 

\section{Limitations}
\subsection{Entity disambiguating}
A problem that occurs when constructing graphs is how to disambiguate the nodes. For example, assume there is a graph consisting of nodes that represents individual identities. Suppose that there exists a node with the label Anders Svensson. Next, an article regarding a person with similar name is added to the graph. Entity disambiguation is required to determine if the article refers to the currently existing Anders or if a new node should be created. Disambiguating entities is important in the construction of the graph, as false results are likely to occur otherwise. Meltwater has an NLP-department which already disambiguates entities with a good performance. Therefore, it is assumed that the entity disambiguation, already in place by Meltwater, is correct. 