\chapter{Methods}

\todo{What tempus should this be in? Past or future?}

A lot of work on large graphs and the approximate neighborhood function has been done by P. Boldi et. al. with the webgraph framework\cite{webgraph}. The framework will be studied extensively to understand the methods used. As the framework only support static graphs, there are no way of changing the webgraph graphs at all. Extensions to the framework will be made to support dynamic graphs and an evolving approximate neighborhood function. 

The research will be mainly about the compression techniques of webgraph graphs \cite{webgraph-compression} and the HyperBall algorithm \cite{hyperball} including HyperLogLog counters \cite{hyperloglog}. Some research in general dynamic algorithms will also be made.

\section{Extending HyperLogLog to support deletions}
There has to be a way to remove elements from the HyperLogLog counter as nodes in the graph can be deleted. A way to delete an element from the counter while preserving the accuracy should be proposed. If there is no time to accomplish this or the results are not sufficiently accurate we could forbid removing nodes. This seems like a drastic choice to make but many data-streams are of the type "append-only", in which the resulting graph only admits node additions.

To be able to manipulate the HyperLogLog counters in a way that they preserves the current precision, we must first investigate the current implementation and understand how additions are currently accounted. This gives insight into how to efficiently decrease the counters without altering their accuracy.

\section{Merging graphs}

The compression methods used are very sensitive to changes. Nodes can copy neighbors from other nodes so if the neighbors of a node change we also have to find all nodes that reference the modified node and update them as well. Moreover, the graph is either stored in a file or in a byte buffer. None of these support insertion of data at an arbitrary position without expensive reallocation and repositioning of the data succeeding that position. 

As we require dynamic graphs, the optimal method of merging two graphs without taking too much time should be investigated. This may include ignoring some compression methods if it speeds up the merge significantly.

\section{Extending HyperANF to support dynamic graphs}
Our proposed algorithm should include two phases. In the first phase, we will run HyperANF on the current state of the graph. The second phase is the dynamic part which starts immediately after HyperANF has completed its calculations.  In the second phase the algorithm should take the information produced by HyperANF and modify it when new nodes are added or deleted. We see two possible implementations for this. The algorithm can either make the appropriate changes for every node added/deleted or buffer up a sub-graph, run an algorithm on this sub-graph, and then merge it with the larger graph. The second approach will be faster as nodes can share results . However, it is not certain that the second approach will be as precise as the first approach. 

To update the neighborhood function we must study approaches to update nodes in the graph efficiently. The first problem is, which nodes are affected by a change somewhere in the graph? Can we create some hierarchy to identify which parts of the graph we need to search for nodes to update? 

\section{Tracking changes over time}
Our algorithm should be able to track changes over time. To do this we have to keep track of previous data and analyse the change over time. This may not fit in the memory of a computer. Then, effective ways of storage has to be investigated.

\section{Benchmarks} 
Our proposed algorithm should be compared to the static algorithms of ANF by letting the static version perform a complete recalculation of a dynamic graph. Then we are able to see how many updates we can perform in the dynamic ANF in the time it takes for the static ANF to complete. For small data-sets it might also be possible to compare our algorithm to an exact computation. For other researches to be able to compare their algorithm to our, the data-sets used to perform the benchmarks presented in the final thesis will be publicly available. We will also compare the accuracy of our proposed algorithm with HyperANF. 





