\chapter{Discussion}
With the DANF algorithm it is now possible to continuously maintain an approximate neighborhood function for all nodes in a graph that supports insertions. As node centralities now can be tracked over time, graphs can be further mined for data and trends can be detected. 

DANF is more efficient than a HyperANF recalculation when the edge insertions bulk size is small compared to the total size of the graph. Further optimizations will increase the time gap. However, DANF use more memory than HyperANF, so a HyperANF recalculation might prove to be able to handle larger graphs given a certain amount of RAM. 

An unexpected drawback in the algorithm is the space used by MS-BFS. It has a space complexity of $O(ns)$, where $s$ is the number of sources, and in practice it used a lot more storage than anticipated. The percentage of storage used by MS-BFS is presented in table \ref{table:ms-bfs_space}. See Appendix \ref{appendix4} for graph details. The extra memory means that DANF can use 4 times as much space as HyperBall even when the history counters are not included. However, the MS-BFS space percentage tends to decrease as the graph size increase. The MS-BFS, vertex cover and HyperLogLog counters are included in the total algorithm space.

\begin{table}[h]
    \center
    \begin{tabular}{ | l | l | l | l |}
        \hline
        edges & uk-2007-05@100000 & in-2004 & it-2004 \\ \hline
        640  & 56.1\% & 40.3\% & 23.5\% \\ \hline
        2560 & 72.3\% & 65.8\% & 29.1\% \\ \hline 
        4480 & 75.6\% & 71.7\% & 37.8\% \\
        \hline
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{Percentage of the algorithm space used by MS-BFS with h = 3 and 16 HyperLogLog register per node}
    \label{table:ms-bfs_space}
\end{table}

\section{Ethics}

Internet can be represented as an undirected graph. Routers can be represented as nodes and connections between routers as edges. Applying the neighbourhood function to this graph can help identifying the most central routers. Assume a hacker get hold of this data. The hacker can then target attacks on only these routers to disrupt the maximum amount of people, with minimal effort. Applying the neighbourhood function to the Internet graph can also be used to simulate an attack. This can be achieved by first running ANF, then deleting some nodes or edges and then running ANF again. The difference in ANF values give insight of how the deletions affected the graph. With the Internet graph, this predicts the efficiency of targeting certain routers. 

The exact same information above can be used by safety companies to prevent attacks. By identifying critical routers extra precautions can be taken at these routers. The information provided by the neighbourhood function can be used for both good and bad purposes, depending on who gets hold of it.

The information retrieved from neighborhood function may also have ethical issues when applied to a graph that includes personal data. Assume a graph where the nodes are journalists and articles. In this graph an edge represents that a journalist have written a certain article or that an article is referenced by another article. By applying the neighborhood function to this graph central journalists and articles can be identified. A central article is referenced in many other articles and a central journalist is a journalist that has written many central articles. This can be a great help when evaluating the credibility of a journalist. It can also be used to retrieve the most influential articles in the graph. The ability to find the most influential journalists must be used with caution. If the graph only consist of articles that are critical to a certain view, the neighbourhood function can identify what journalists to target to silence the criticism. If the influence score is made public, or able to be bought, it could also affect the career of a journalist. If the score is not perfect, an influential journalist could be incorrectly marked as non-influential which might decrease the chance of getting good stories.

The ability to identify central and influential nodes can be useful in many different situations. When the information is used for realisations that benefits the humanity as a whole, it should always be acceptable. An example of this is when it is applied to find information for medical purposes, perhaps by finding out central genes that are involved in the development of cancer. This should be helpful for all of humanity. Using the information for selfish purposes requires more investigation to determine if it is acceptable. Retrieving personal data may in some cases be fine, but in many other cases should be strictly forbidden. It all depends on the application of whether the information from the neighborhood function should be allowed to use. In general, if everyone benefits from it, it should be fine to use.
 
\section{Future work}
\label{sec:future_work}
DANF is currently not a fully dynamic algorithm as deletions in the graph are not completed. As this is part of the problem description, the next natural step is to fully implement deletions. There are two missing implementations to achieve deletions, removing edges from a graph file and decreasing HyperLogLog counters affected by a deletion. Deleting edges from a graph file can be performed by re-storing the graph, but a better solution may be to temporarily keep track of which edges are deleted and ignore these until a store is performed. Decreasing HyperLogLog counters can be implemented by a two step algorithm. First, perform a BFS in the transpose of the graph to find all nodes that are possibly affected by the deletion. Then, each possibly affected node can be treated as a new node which will recalculate the node counters. This can be implemented in $O(m^2)$ time, where $m$ is the number of edges. Another way to decrease the counters is to probabilistically decrease the counters of all affected nodes. This would speed up the algorithm but can affect the precision of the counters.

DANF also have to be further optimized. Due to time limitations there exists unoptimized bottlenecks. An improvement would be to make the program execute more in parallel. With the exception of the MS-BFS, the program is run sequentially. 

Another improvement would be to separate responsibilities among machines. Currently, a single machine will update and save the graph, calculate node counters and keep track of top counters. These tasks can be divided among several machines, hence speeding up the algorithm and making it more scalable. Also, the main part of the algorithm, the MS-BFS, could be distributed. Making the MS-BFS scale with the number of machines greatly increases the potential of the algorithm. There are already implementations of standard BFS on several machines. Adapting those implementations to handle MS-BFS should be possible. One tool that can be used is Giraph \cite{giraph}. Giraph is an open-source implementation of a vertex-centric distributed graph processing system.