\chapter{Technical Background}

\section{Neighborhood function}
The individual neighborhood function $(IN)$ takes a directed and unweighted graph $G = (V,E)$, a node $v \in V$, and a number of steps $h \in \mathbb{N}$, and calculates which nodes $v$ can reach in $h$ steps in the graph, that is, $IN(G,v,h) = \{u | u \in V, dist(v,u) \leq h\}$, where $dist(v,u)$ is the length of the shortest path from $v$ to $u$. A brute force algorithm for computing $IN(G,v,h)$ exactly is to perform a breadth-first search that saves every encountered node within h levels. The resulting set will represent $v$'s individual neighborhood.

The neighborhood function $(NF)$ calculates the individual neighborhood function for all nodes $v \in V$. Formally speaking, the neighborhood function calculates $NF(G,h) = \{ IN(G,v,h) | v \in V \}$.

\section{HyperANF}
HyperANF is an algorithm which calculates the approximate size of the individual neighborhood function for all nodes in a graph simultaneously. The algorithm works in the following way: In the first iteration, let the set of reachable nodes from each node $v$ be $R_0(v) = \{v\}$. Then, for $1 \leq i \leq h$ iterations, let each node take the union of all its neighbors sets $R_i(v) = R_{i-1}(v) \bigcup\limits_{u \in s(v)} R_{i-1}(u) $ where $s(v) = \{u | (v,u) \in E\}$. After $h$ steps every nodes reachability set will contain the nodes it can reach in $h$ steps \cite{hyperball}. However, instead of keeping track of the set of reachable nodes, HyperANF uses one HyperLogLog counter per node to approximately count the size of the set of the reachable nodes. After $h$ iterations, each node will have an approximate number of nodes it can reach in $h$ steps. The benefit of calculating the set sizes approximately is that it only requires $O(n\log\log n)$ units, in contrast to the exact algorithm which requires $O(n^2)$.

\section{HyperLogLog}
HyperLogLog \cite{hyperloglog} is the state-of-the-art cardinality estimator of multisets that are received as a data stream. A multiset $M$ is an arbitrarily ordered set that allows duplicate elements. HyperLogLog takes a multiset in the form of a data stream $S$ and estimates the number of unique elements.

HyperLogLog works by hashing the values seen in the data stream and storing the maximum number of leading consecutive zeroes in the hash. The intuition of the algorithm is that hashes with few consecutive zeroes are more likely to appear than hashes with many consecutive zeroes. An assumption of the algorithm is that the hashed values are evenly distributed across the target domain, regardless of the original value, to make every bit of the hash have an equal probability of being zero or one. The probability of only one consecutive zero is $\frac{1}{2}$, the probability of two zeroes is $\frac{1}{2}*\frac{1}{2}$ and the probability of $k$ zeroes are $\frac {1} {2^k}$. Assume $p-1$ consecutive zeroes and a one have been seen. The probability for this to happen is $\frac{1}{2^p}$. Then, in expectation, $2^p$ unique hashes should have been seen and it can approximately be concluded that roughly $2^p$ unique elements have been seen. For the proof of this, see Appendix \ref{appendix1}. To improve the precision of the algorithm several registers are used to average over. 

\subsection{Memory usage}
An intuitive algorithm for counting cardinality exactly would be to save each item received in the data stream into a set S. The cardinality would then be $|S|$. With this intuitive algorithm $O(n)$ space is needed to calculate a cardinality of size n. For various applications this space complexity is not sufficiently low. Therefore, space saving approximation algorithms, such as HyperLogLog, have been developed. With a space complexity of $O(\log\log n)$ HyperLogLog has the lowest space complexity of all practical approximate cardinality estimators. 

Given that the actual cardinality of the input is $n$, the hash function needs to target a domain of size at least $n$. This to avoid hash collisions. The maximum numbers of zeroes in sequence is the number of bits in $n$, which is $\log_2(n)$. For example, if $n=64$, the number to store would be a maximum of 6. To store a number of size $\log_2(n)$, $\log_2\log_2(n)$ bits is needed. If $n=64$ and the max number to store is 6, only 3 bits are needed. If $n=10^9$, only 5 bits are needed. This leads to the space complexity for this algorithm, which is $O(\log\log n)$ \cite{hyperloglog}. 

\subsection{Multiple register}
Registers are the locations where the number of trailing zeroes is stored. If only one register is used, the precision is very bad. The precision can be drastically improved by having several registers and later calculating the mean of the estimates. Any given number must always be placed in the same register. Adding a number that has been added before should not affect the cardinality. To solve this, the first $b$ bits of the hash is used as a register index. This means that the number of registers will be $2^b$. By using more registers, a higher precision is achieved but it also implies a higher space and time consumption. The space complexity is $O(2^b\log\log n)$ \cite{hyperloglog}. 

\subsection{Harmonic means}
When the cardinality is calculated the mean of all the registers are used. It may be that some element in the registers become very large even though the number of elements is quite small. This is a problem which arise quite often and to deal with this Harmonic means are used. One important property of harmonic means is that large values have less leverage than small values. The mathematical property is $\min(x_1,x_2,..) \leq H(x_1,x_2,..) \leq n*\min(x_1,x_2,..)$ where $n$ is number of elements. This prevents the mean to spike off when there is a few large values.  

\subsection{Precision}
HyperLogLog is a probabilistic algorithm and understanding its performance require a deep analysis. Only the final conclusions are presented below. For a full proof see \cite{hyperloglog}. 

Let $E$ be the estimate produced by the mean of all the registers, $\mathbb{E}_n(E)$ be the expectation of $E$ with an unknown cardinality $n$, and $m$ be the number of registers used per counter. Assuming $m \geq 16$ , the relation $\frac{1}{n}*\mathbb{E}_n(E) = 1 + \delta_1(n) + o(1)$ holds. $\delta_1$ is a small oscillating function with $|\delta_1(n)| < 5*10^{-5}$, and has no significant impact on the expected value. This indicates that $E$ is an asymptotically almost unbiased estimator.

Flajolet et al. also provides the standard error \cite{hyperloglog}. Let $\mathbb{V}_n(E)$ be the variance of $E$ with an unknown cardinality $n$. Assuming $m \geq 16$,  $\frac{1}{n}*\sqrt{\mathbb{V}_n(E)} = \frac{\beta_m}{\sqrt{m}}  + \delta_2(n) + o(1)$. $\delta_2$ is an oscillating function with $|\delta_2(n)| < 5*10^{-4}$, and $\beta_m$ is a constant only depending on $m$. For $m = 16$, $\beta_m = 1.106$ and for increasing $m$, $\beta_m$ decreases asymptotically toward $1.03896$. It follows that the precision of each sample of the counter depends on the number of registers in the counter. By choosing a large number of registers a better precision is achieved. Chassaing and GÃ©rin proved a lower bound for this approximation problem to be $\frac{1}{\sqrt{m}}$. This shows that this algorithm is nearly optimal \cite{nearopt}. Flajolet et al. compares the precision of HyperLogLog and exact measures to confirm that the standard error is low \cite{hyperloglog}.

\subsection{Hashing function}
For the HyperLogLog algorithm to work, it is necessary that every bit in the hash value has equal probability of occurring \cite{hyperloglog}. This can be achieved by a hashing function with good avalanche property. The avalanche effect is when changing a single bit in the input leads to a very different output. In addition, for HyperLogLog to be useful for data streams with very large cardinality the hashing function must also be exceptionally fast. HyperBall, an implementation of HyperANF, uses the hashing function Jenkins \cite{webgraph}. Experimental results shows that the Jenkins hash exhibits a good avalanche property. It is also very fast to compute as it only uses a few clock cycles for start up and about one clock cycle per three bytes of input.

\iffalse
\subsection{BroadWord}
Broadword is a way to do several calculations at the same time using cleverly constructed binary-logic expressions. HyperANF uses broadword to take the union $U$ of two HyperLogLog counters $A$ and $B$, which effectively is taking $\forall i; U_i = max(A_i,B_i)$
\fi

\section{Webgraph}
Webgraph is a framework for compressing and analyzing very large graphs \cite{webgraph}. It is developed by Boldi and Vigna, two of the developers of the HyperANF algorithm.


\section{HyperBall}
HyperBall is an open-source framework that utilizes the HyperANF algorithm for performing computations related to the neighborhood function. It is a part of the webgraph framework.

\section{Graph compression}
\label{sec:Compression}

The webgraph framework \cite{webgraph} uses many different ways to compress a graph but its principal method is the format BVGraph \cite{webgraph-compression}. It exploits common relational properties of the websites when sorted in lexicographic order and given an index based on their position in the order. A relation from page $A$ to page $B$ exists if there is a hyperlink from page $A$ to $B$. One common property is that most hyperlinks on websites are navigational, i.e. leads to a different page on the same website. So if the links are sorted in lexicographic order many nodes indices will be close to each other. This property is called \emph{locality}. Another common pattern is that, as most links are navigational, many nodes close to each other have many common neighbors, called \emph{similarity}. The navigational links often refer to pages far down in the site hierarchy and all of these pages will be right beside each other because of the long mutual address prefix. So many of the neighbors form long consecutive sequences of one-increasing indices, a property called \emph{consecutivity}.

To take advantage of the \emph{locality} property a neighbor in a graph are represented as a difference from the previous neighbor. This makes the numbers significantly smaller and can take up less space. Another compression method called copy-blocks exploits the \emph{similarity} property and copies neighbors from a node nearby. If two nodes have almost the exact same neighbors it saves a lot of space. The \emph{consecutivity} property is used by saving large portions of intervals as just two numbers: the start node and the interval length \cite{webgraph-compression}. 

A way to store integers of varying size needs to be used as the compression uses methods to minimise the size of numbers. There are several methods to do this which have different advantages and disadvantages. Webgraph uses different methods for different usages to optimize the space taken \cite{webgraph-compression}. 

\subsection{Layered Label Propagation}

While the properties mentioned above are trivially achieved in a graph of hyperlinks, they might be harder to find in different types of graphs. To be able to use the compression techniques in other types of graphs P. Boldie et al. have developed a method called LLP (Layered Label Propagation) \cite{llp} to reorder the node indices to use the compressions more efficiently. 

\section{Graph transpose}
The transpose of a graph is a graph with all of its edges flipped. 

\section{Breadth-first search}
Breadth-first search (BFS) is an algorithm which traverses a graph from a given origin node $v$. It starts by adding the node $v$ to a queue. Then, while the queue is not empty, the algorithm takes the first node $u$ from the queue and adds the previously not seen neighbors of $u$ to the queue. In the first step $v$ will be visited. In the second step the neighbors of $v$ will be visited, and so on until all reachable nodes are traversed.

\subsection{Multi source}
There exists algorithms that optimize the calculation of several BFS's simultaneously. These optimization mainly targets the ability of the different searches to use each others computations. The state-of-the-art algorithm for running a multi source breadth-first search on the CPU is called MS-BFS \cite{msbfs}.

The algorithm works by propagating a set of BFSs that have reached the same node at the same time. To begin with, one set is created for every BFS at their respective source node. The sets only contains the BFS that starts at the respective source node. In the next iterations, all sets are propagated to the neighbors of the node they were previously on. If several sets reach a node, the sets are merged. Additionally, the nodes that the BFSs have already reached are tracked and any BFS that reach a node it has already seen is ignored. 

This means that once two breadth first searches meet, they can be considered as one. This decreases the amount of necessary propagations compared to the individual BFS case. The sets of BFSs are lists of bits where the indices of the bits represent the id of the BFSs \cite{msbfs}.

\section{Maximal matching}
Given a graph $G = (V,E)$, a matching is a subset $M \subseteq E$ such that all nodes are incident to at most one edge in $M$. A maximal matching is then a matching such that if another edge is added to it, it is no longer a matching. 

\section{Approximation algorithms}
Approximation algorithms are useful to calculate a non-optimal answer quickly where the optimal solution would take too long to calculate. A C-approximation means that the solution is in worst case C times the size of the optimal solution. For example, if you calculate the minimal vertex cover with a 2-approximation the solution can at most be twice as large as the optimal solution. If you want to calculate the maximal matching with a $\frac{1}{2}$-approximation the size of the solution can be be no less than half of the optimal solution.

\section{Minimum vertex cover}
Given an undirected graph $G = (V,E)$, a vertex cover is a subset $S \subseteq V$ such that $\forall e = (u,v) \in E,  u \in S \vee v \in S$. A minimum vertex cover is a set $S$ of minimum size. The problem of finding a minimum vertex cover is NP-complete \cite{Kar72}.

\subsection{Approximate minimum vertex cover}
As minimum vertex cover is NP-complete there have been studies of polynomial time approximation algorithms. The state-of-the-art algorithms give $2$-approximations. For general graphs it is hard to achieve a $(2-\epsilon)$-approximation for any $\epsilon > 0$ \cite{2-evchard}, so $2$ can be the best constant approximation factor achievable.

A simple greedy $2$-approximation algorithm maintain a maximal matching $M$ to calculate a vertex cover $V$. By the definition of maximal matching it is certain that every edge are either in the maximal matching or shares a node with one in it. For all edges, $e = (u,v) \in M$, pick both $u$ and $v$ for the vertex cover, and all edges are guaranteed to be covered. For an optimal minimum vertex cover $V_{opt}$, it holds that $\forall e = (u,v) \in M, u \in V_{opt} \vee v \in V_{opt}$. This can be proved by a simple contradiction. Assume $u,v \notin V$. This implies that $e$ is not covered by any node, hence $V$ is not a vertex cover $\blacksquare$. So by picking both nodes for all edges in the maximal matching, a $2$-approximation algorithm is achieved as at least one of the nodes must be in $V_{opt}$.

\subsection{Dynamic approximate minimum vertex cover}
There exists several different algorithms for maintaining a fully dynamic approximate vertex cover \cite{2appdynvc, 2appdynvclogn, 2eappdynvc}. A fully dynamic algorithm supports both insertions and deletions. The main difference between the algorithms is the trade off between the time complexity for insertions and deletions. The state-of-the-art fully dynamic approximate minimum vertex cover algorithms can achieve $2$-approximations.




