\chapter{Technical Background}

\section{Neighborhood function}
The individual neighborhood function $(IN)$ takes a directed and unweighted graph $G = (V,E)$, a node $v \in V$, and a number of steps $h \in \mathbb{N}$, and returns the number of nodes $v$ can reach in $h$ steps in the graph, that is, $IN(G,v,h) = |\{u | u \in V, dist(v,u) \leq h\}|$, where $dist(v,u)$ is the length of the shortest path from $v$ to $u$. A brute force algorithm for computing $IN(G,v,h)$ exactly is to perform a breadth-first search that saves every encountered node within $h$ levels. The cardinality of the resulting set will represent $v$'s individual neighborhood.

The neighborhood function $(NF)$ returns the individual neighborhood function for all nodes in the graph. Formally speaking, the neighborhood function is defined as: $NF(G,h) = \{ IN(G,v,h) | v \in V \}$.

\section{HyperANF}
HyperANF is an algorithm which calculates the approximate neighborhood function for a graph. The algorithm works in the following way: In the first iteration, let the set of reachable nodes from each node $v$ be $R_0(v) = \{v\}$. Then, for $1 \leq i \leq h$ iterations, let each node take the union of all its neighbors sets $R_i(v) = R_{i-1}(v) \bigcup\limits_{u \in s(v)} R_{i-1}(u) $ where $s(v) = \{u | (v,u) \in E\}$. After $h$ steps every nodes reachability set will contain the nodes it can reach in $h$ steps \cite{hyperball}. However, instead of keeping track of the set of reachable nodes, HyperANF uses one HyperLogLog counter per node to approximately count the size of the set of the reachable nodes. The benefit of calculating the set sizes approximately is that it only requires $O(n\log\log n)$ units, in contrast to the exact algorithm which requires $O(n^2)$.

\section{HyperLogLog}
HyperLogLog \cite{hyperloglog} is the state-of-the-art cardinality estimator of multisets that are received as a data stream. A multiset $M$ is an arbitrarily ordered set that allows duplicate elements and the cardinality is the number of distinct elements. HyperLogLog takes a multiset in the form of a data stream $S$ and estimates the cardinality.

HyperLogLog works by hashing the elements seen in the data stream and storing the maximum number of leading consecutive zeroes in the bits of the hash. The intuition of the algorithm is that hashes with few leading zeroes are more likely to appear than hashes with many leading zeroes. A crucial assumption of the algorithm is that the hashed elements are evenly distributed across the target domain. Hence, every bit of the hash sequence has an equal probability of being zero or one. The probability of only one consecutive zero is $\frac{1}{2}$, the probability of two zeroes is $\frac{1}{2}*\frac{1}{2}$ and the probability of observing $k$ zeroes is $\frac {1} {2^k}$. Assume $p-1$ consecutive zeroes and a one have been seen. The probability of this outcome is $\frac{1}{2^p}$. The expected number of tries for this probability to occur is $2^p$ and it can be concluded that approximately $2^p$ unique elements have been seen in the multiset \cite{hyperloglog}. 
\subsection{Terminology}
\textbf{Added }
Adding an element to a HyperLogLog counter means to hash the element, count the consecutive leading zeroes and store that number in the counter if it is larger than the existing value.\\\\
\textbf{Included }
An element is said to be included in a HyperLogLog counter if it has been added to it.\\\\
\textbf{Union }
The union of two HyperLogLog counters is the same as adding all elements included in both counters in to one counter. This is performed by taking the maximum of each saved value \cite{hyperanf}.\\\\
\textbf{Subcounter }
A counter $A$ is a subcounter of $B$ if all elements included in $A$ are included in $B$. Subcounter is denoted by $\subseteq$ and $\subset$.

\subsection{Multiple register}
In HyperLogLog counters, registers are the locations where the number of trailing zeroes is stored. If only one register is used, the precision is very bad. The precision can be drastically improved by having several registers and later calculating the mean of the estimates. Any given number must always be stored in the same register each time so that adding a number that has been added before does not affect the cardinality. To solve this, the first $b$ bits of the hash are used as a register index. This means that the number of registers will be $2^b$. By using more registers, a higher precision is achieved, but the time and space requirement is increased. The space complexity is $O(2^b\log\log n)$ \cite{hyperloglog}.

The algorithm with multiple registers works as following: Given a number $x$ as input; Run the hash function on $x$ so that $x_h = hash(x)$. Remove the $b$ least significant bits from $x_h$ and call them $r$. Let the number of consecutive trailing zeroes in the remaining bits of $x_h$ be called $c_0$. If the number in register $r$ is less than $c_0 + 1$: store $c_0 + 1$ in register $r$ \cite{hyperloglog}.

\subsection{Memory usage}
An intuitive algorithm for calculating cardinality exactly is to save each element received in the data stream into a set $S$. Then the cardinality will be $|S|$. With this intuitive algorithm $O(n)$ space is needed to calculate the cardinality of a set with size $n$. For various applications, this space complexity is not sufficiently low. Therefore, space saving approximation algorithms, such as HyperLogLog, have been developed. HyperLogLog has a space complexity of $O(\log\log n)$. There is an, in theory, optimal cardinality estimator \cite{optimal_cardinality}. However, that algorithm is complex and an implementation is likely impractical \cite{google_hyperloglog}. 

Given that the actual cardinality of the input is $n$, the hash function needs to target a domain of size at least $n$. This reduces the number of hash collisions. Only the number of leading consecutive zeroes in the hash is stored. The maximum number of zeroes in the hash sequence is the number of bits required to represent a number $n$, which is equal to $\log_2(n)$. For example, if $n=64$, the number to store would be at most 6. To store a number of size $\log_2(n)$, $\log_2\log_2(n)$ bits are needed. If $n=64$ and the max number to store is 6, only 3 bits are needed. If $n=10^9$ only 5 bits are needed. This leads to the space complexity of $O(\log\log n)$ for this algorithm \cite{hyperloglog}. 


\subsection{Harmonic means}
The harmonic mean of all the registers is used to calculate the cardinality. An important property of harmonic means is that extreme values have less leverage than small ones. This property is mathematically written as $\min(x_1,x_2,..) \leq H(x_1,x_2,..) \leq n*\min(x_1,x_2,..)$ where $n$ is number of elements. This prevents the mean to spike off when there are a few extreme values. The harmonic mean is defined as: $\dfrac{n}{\frac{1}{x_1}+\frac{1}{x_2}+...+\frac{1}{x_n}}$ 

\subsection{Precision}
HyperLogLog is a probabilistic algorithm and understanding its performance require a deep analysis. Only the final conclusions are presented below. For a full proof see \cite{hyperloglog}. 

Let $E$ be the estimate produced by the mean of all the registers, $\mathbb{E}_n(E)$ be the expectation of $E$ with an unknown cardinality $n$, and $m$ be the number of registers used per counter. Assuming $m \geq 16$ , the relation $\frac{1}{n}*\mathbb{E}_n(E) = 1 + \delta_1(n) + o(1)$ holds. $\delta_1$ is a small oscillating function with $|\delta_1(n)| < 5*10^{-5}$, and has no significant impact on the expected value. This indicates that $E$ is an asymptotically almost unbiased estimator.

Flajolet et al. also provides the standard error \cite{hyperloglog}. Let $\mathbb{V}_n(E)$ be the variance of $E$ with an unknown cardinality $n$. Assuming $m \geq 16$,  $\frac{1}{n}*\sqrt{\mathbb{V}_n(E)} = \frac{\beta_m}{\sqrt{m}}  + \delta_2(n) + o(1)$. $\delta_2$ is an oscillating function with $|\delta_2(n)| < 5*10^{-4}$, and $\beta_m$ is a constant only depending on $m$. For $m = 16$, $\beta_m = 1.106$ and for increasing $m$, $\beta_m$ decreases asymptotically toward $1.03896$. It follows that the precision of each sample of the counter depends on the number of registers in the counter. By choosing a large number of registers a better precision is achieved. Chassaing and GÃ©rin proved a lower bound for this approximation problem to be $\frac{1}{\sqrt{m}}$. This shows that this algorithm is nearly optimal \cite{nearopt}. Flajolet et al. compares the precision of HyperLogLog and exact measures to confirm that the standard error is low \cite{hyperloglog}.

\subsection{Hashing function}
For the HyperLogLog algorithm to work, it is necessary that every bit in the hash value have equal probability of occurring \cite{hyperloglog}. This can be achieved by a hashing function with good avalanche property. The avalanche effect is when changing a single bit in the input leads to a very different output. In addition, for HyperLogLog to be useful for data streams with very large cardinality the hashing function must also be fast. HyperBall, an implementation of HyperANF, uses the hashing function Jenkins \cite{webgraph}. Experimental results show that the Jenkins hash exhibits a good avalanche property. It is also very fast to compute as it only uses a few clock cycles for start up and about one clock cycle per three bytes of input.

\iffalse
\subsection{BroadWord}
Broadword is a way to do several calculations at the same time using cleverly constructed binary-logic expressions. HyperANF uses broadword to take the union $U$ of two HyperLogLog counters $A$ and $B$, which effectively is taking $\forall i; U_i = max(A_i,B_i)$
\fi

\section{Webgraph}
Webgraph is a framework for compressing and analyzing very large graphs \cite{webgraph}. It is developed by Boldi and Vigna, two of the developers of the HyperANF algorithm.


\section{HyperBall}
HyperBall is an open-source framework that utilizes the HyperANF algorithm for performing computations related to the neighborhood function. It is a part of the webgraph framework.

\section{Graph compression}
\label{sec:Compression}

The webgraph framework \cite{webgraph} uses many different ways to compress a graph but its principal method is the format BVGraph \cite{webgraph-compression}. It exploits common relational properties of the websites when sorted in lexicographic order and given an index based on their position in the order. A relation from page $A$ to page $B$ exists if there is a hyperlink from page $A$ to $B$. One common property is that most hyperlinks on websites are navigational, i.e. leads to a different page on the same website. So, if the links are sorted in lexicographic order, many node indices will be close to each other. This property is called \emph{locality}. Another common pattern is that, as most links are navigational, many nodes close to each other have many common neighbors. This property is called \emph{similarity}. The navigational links often refer to pages far down in the site hierarchy and all of these pages will be right beside each other because of the long mutual address prefix. Hence, many of the neighbors form long consecutive sequences of one-increasing indices, a property called \emph{consecutivity}.

To take advantage of \emph{locality}, the neighbors are encoded as the difference from the neighbor preceding it in the neighbor list. This makes the encoded numbers significantly smaller and can take up less space. Another compression method called copy-blocks exploits the \emph{similarity} property and copies neighbors from a node nearby. This saves a lot of space if two nodes have almost the exact same neighbors. The \emph{consecutivity} property is used by saving large portions of intervals as just two numbers: the start node and the interval length \cite{webgraph-compression}. 


\subsection{Layered Label Propagation}

While the properties mentioned above are trivially achieved in a graph of hyperlinks, they might be harder to find in other types of graphs. To be able to use the compression techniques in other types of graphs P. Boldie et al. have developed a method called LLP (Layered Label Propagation) \cite{llp} to reorder the node indices to use the compression techniques more efficiently. 

\section{Breadth-first search}
Breadth-first search (BFS) is an algorithm which traverses a graph from a given origin node $v$. It starts by adding the node $v$ to a queue. Then, while the queue is not empty, the algorithm takes the first node $u$ from the queue and adds the previously not seen neighbors of $u$ to the queue. In the first step, $v$ will be visited. In the second step the neighbors of $v$ will be visited, and so on until all reachable nodes are traversed.

\subsection{Multi source}
There exists algorithms that optimize the calculation of several BFS's simultaneously. These optimization mainly targets the ability of the different searches to use each others computations. The state-of-the-art algorithm for running multiple breadth-first searches on the CPU is called Multi Source Breadth-First Search (MS-BFS) \cite{msbfs}.

MS-BFS works by propagating a set of BFSs that have reached the same node at the same time. To begin with, one set is created for every BFS at their respective source node. The sets only contains the BFS that starts at the respective source node. In the next iterations, all sets are propagated to the neighbors of the node they were previously on. If several sets reach a node, the sets are merged. Additionally, the nodes that the BFSs have already reached are tracked and any BFS that reach a node it has already seen is ignored. 

This means that once two breadth first searches meet, they can be considered as one. This decreases the amount of necessary propagations compared to the individual BFS case. The sets of BFSs are lists of bits where the indices of the bits represent the id of the BFSs \cite{msbfs}.

\section{Approximate minimum vertex cover}
As minimum vertex cover is NP-complete there have been studies of polynomial time approximation algorithms. The state-of-the-art algorithms give $2$-approximations. For general graphs it is hard to achieve a $(2-\epsilon)$-approximation for any $\epsilon > 0$ \cite{2-evchard}, so $2$ can be the best constant approximation factor achievable.

A simple greedy $2$-approximation algorithm maintains a maximal matching $M$ to calculate a minimal vertex cover $V$. By the definition of maximal matching it is certain that every edge is either in the maximal matching or shares a node with one in it. For all edges $e = (u,v) \in M$, pick both $u$ and $v$ for the vertex cover, and all edges are guaranteed to be covered. For an optimal minimum vertex cover $V_{opt}$, it holds that for all edges $e = (u,v) \in M, u \in V_{opt} \vee v \in V_{opt}$. This can be proved by a simple contradiction. Assume $u,v \notin V$. This implies that $e$ is not covered by any node, hence $V$ is not a vertex cover $\blacksquare$. So by picking both nodes for all edges in the maximal matching, a $2$-approximation algorithm is achieved as at least one of the nodes must be in $V_{opt}$.

\subsection{Dynamic approximate minimum vertex cover}
There exists several different algorithms for maintaining a fully dynamic approximate vertex cover \cite{2appdynvc, 2appdynvclogn, 2eappdynvc}. A fully dynamic algorithm supports both insertions and deletions. The main difference between the algorithms is the trade off between the time complexity for insertions and deletions. The state-of-the-art fully dynamic approximate minimum vertex cover algorithms can achieve $2$-approximations.




