\chapter{Technical Background}

\section{HyperLogLog}
HyperLogLog is the state-of-the-art approximate cardinality estimator of multisets that are received as a data stream. It works by hashing the values seen in the data stream and stores the maximum number of leading subsequent zeroes in the hash. The intuition of the algorithm is that hashes with few subsequent zeroes are more likely to appear than hashes with many subsequent zeroes. An assumption of the algorithm is that the hashed values are evenly distributed across the target domain, regardless of the original value, to make every bit of the hash have an equal chance of being zero or one. The chance of only one subsequent zero is $\frac{1}{2}$, the chance of two zeroes is $\frac{1}{2}*\frac{1}{2}$ and the chance of $k$ zeroes are $\frac {1} {2^k}$. Assume $p-1$ consecutive zeroes and one one have been seen. The chance for this to happen is $\frac{1}{2^p}$. Then, in expectation, $2^p$ unique hashes should have been seen and it can approximately be concluded that roughly $2^p$ unique elements have been seen. For the proof of this, see Appendix \ref{appendix1}. As this is a probabilistic algorithm it might be a very bad approximation. To improve the precision several registers are used to average over. \cite{hyperloglog}

\subsection{Memory usage}
An intuitive algorithm for counting cardinality exactly would be to save each item received in the data stream into a set S. The cardinality would then be $|S|$. With this intuitive algorithm $O(n)$ space is needed to calculate a cardinality of size n. For various applications this space complexity is not sufficiently low. Therefore, space saving approximation algorithms, such as HyperLogLog, have been developed. With a space complexity of $O(\log\log n)$ HyperLogLog has the lowest space complexity of all practical approximate cardinality estimators. 

Given that the actual cardinality of the input is $n$, the hash function needs to target a domain of size at least $n$. This to avoid hash collisions. The maximum numbers of zeroes in sequence is the number of bits in $n$, which is $\log_2(n)$. For example, if $n=64$, the number to store would be a maximum of 6. To store a number of size $\log_2(n)$, $\log_2(\log_2(n))$ bits is needed. If $n=64$ and the max number to store is 6, only 3 bits are needed. If $n=10^9$, only 5 bits are needed. This leads to the space complexity for this algorithm, which is $O(\log\log n)$. \cite{hyperloglog}

\subsection{Multiple register}
Registers are the locations where the number of trailing zeroes is stored. If only one register is used, the precision is very bad. The precision can be drastically improved by having several registers and later calculating the mean of the estimates. Any given number must always be placed in the same register. Adding a number that has been added before should not affect the cardinality if added again. To solve this, the first $b$ bits of the hash is used as a register index. This means that the number of registers will be $2^b$. By using more registers, a higher precision is achieved but it also implies a higher space and time consumption. The space complexity is $O(2^b\log\log n)$. \cite{hyperloglog} 

\subsection{Harmonic means}
When the cardinality is calculated the mean of all the registers are used. It may be that some element in the registers become very large even though the number of elements is quite small. This is a problem which arise quite often and to deal with this Harmonic means are used. One important property of harmonic means is that large values have less leverage than small values. The mathematical property is $\min(x_1,x_2,..) \leq H(x_1,x_2,..) \leq n*\min(x_1,x_2,..)$ where $n$ is number of elements. This prevents the mean to spike off when there is a few large value.  

\subsection{Precision}
HyperLogLog is a probabilistic algorithm and requires a deep analysis to see how well it performs. Only the conclusions are presented below, for a full proof see the original article. 

Let $E$ be the estimate produced by the mean of all the registers, $\mathbb{E}_n(E)$ to be the expectation of E with an unknown cardinality $n$, and $m$ be the number of registers used per counter. Assuming $m \geq 16$ , $\frac{1}{n}*\mathbb{E}_n(E) = 1 + \delta_1(n) + O(1)$, which holds as $n \rightarrow \infty$. $\delta_1$ is a small oscillating function, where $|\delta_1(n)| < 5*10^{-5}$, and has no significant impact on the expected value. This indicates that $\mathbb{E}_n(E)$ is an asymptotically almost unbiased expected value, which means that in average the counter will be correct. 

Flajolet et al. also proves the standard error \cite{hyperloglog}. Let $\mathbb{V}_n(E)$ be the variance of E with an unknown cardinality n. Assuming $m \geq 16$,  $\frac{1}{n}*\sqrt{\mathbb{V}_n(E)} = \frac{\beta_m}{\sqrt{m}}  + \delta_2(n) + O(1)$. $\delta_2$ is an oscillating function, where $|\delta_2(n)| < 5*10^{-4}$, and $\beta_m$ is value dependent on m. For $m = 16$ $\beta_m = 1.106$ and for increasing m, $\beta_m$ decreases asymptotically towards $1.03896$. It follows that the precision from each sample of the counter value depends on the number of registers. By choosing a large number of registers a better precision is achieved in each sample. According to Flajolet et al, Chassaing and GÃ©rin proved a lower bound for this approximation to be $\frac{1}{\sqrt{m}}$, which proves that this algorithm is very near what is optimally possible \cite{nearopt}.

\section{Neighborhood function}
The individual neighborhood function $IN$ is a function that takes a directed and unweighted graph $G = (V,E)$, a node $v \in V$, and a number of steps $h \in \mathbb{N}$ and calculates which nodes $v$ can reach in $h$ steps in the graph. That is, $IN(G,v,h) = \{u | u \in V, dist(v,u) < h\}$ where $dist(v,u)$ is the length of the shortest path from $v$ to $u$. A brute force algorithm for computing $IN(G,v,h)$ exactly would be to perform a breadth-first search that saves every encountered node within h levels. The resulting set will represent $v$'s individual neighborhood.

The neighborhood function $N$ is a function that calculates the individual neighborhood function for each node $v \in V$. Formally, the neighborhood function calculates $N(G,h) = \{ IN(G,v,h) | v \in V \}$.


\section{HyperANF}
HyperANF is an algorithm which calculates the approximate size of the individual neighborhood function for all nodes in a graph simultaneously. The algorithm is in theory simple and works roughly in the following way: In the first iteration, let the set of reachable nodes from each node $v$ be $R_0(v) = \{v\}$. Then, for $1 \leq i \leq h$ iterations, let each node take the union of all its neighbors sets $R_i(v) = R_{i-1}(v) \bigcup\limits_{u \in s(v)} R_{i-1}(u) $. After $h$ steps every nodes reachability set will contain the nodes it can reach in $h$ steps \cite{hyperball}. However, instead of keeping track of the set of reachable nodes, HyperANF uses one HyperLogLog counter per node to approximately count the size of the set of the reachable nodes. Then, instead of taking the union of all neighbors sets HyperANF takes the union of the counters. After $h$ iterations, each node will have an approximate counter of how many distinct nodes it can reach in $h$ steps. The benefit of calculating the set sizes approximately is that it only requires $O(n\log\log n)$ space, in contrast to the exact algorithm which requires $O(n^2)$.


\subsection{Hashing function}
For the HyperLogLog algorithm to work it is necessary for the hash function to have the property that every bit in the hash value has equal chance of occurring \cite{hyperloglog}. This can be achieved by a hashing function with good avalanche effect. The avalanche effect is that when you change one bit in the input the output is very different. This prevents similar input values to be mapped to the same output. In addition, for HyperLogLog to be useful for data streams with very large cardinality the hashing function must also be exceptionally fast. HyperANF uses the hashing function Jenkins. Experimental results shows that the Jenkins hash exhibits good avalanche effects. It is also very fast to compute as it only uses a few clock cycles for start up and about one clock cycle per three bytes of input.

\iffalse
\subsection{BroadWord}
Broadword is a way to do several calculations at the same time using cleverly constructed binary-logic expressions. HyperANF uses broadword to take the union $U$ of two HyperLogLog counters $A$ and $B$, which effectively is taking $\forall i; U_i = max(A_i,B_i)$
\fi


\section{HyperBall}
HyperBall is an open-source framework that utilizes the HyperANF algorithm for calculating neighborhood related computations. It was developed by Boldi and Vigna, two of the researchers for the HyperANF algorithm.

\section{Breadth-first search}

Breadth-first search is an algorithm which traverses a graph from a given origin vertex $v$. It starts by adding the node $v$ to a queue. Then, while the queue is not empty, the algorithm takes the first node $u$ from the queue and adds the previously not seen neighbours of $u$ to the queue. In the first step $v$ will be visited. In the second step the neighbors of $v$ will be visited, and so on until all reachable nodes are traversed.

\subsection{Multi source}
When it is necessary or desirable to calculate several breadth-first searches simultaneously there exists algorithms that optimize these searches. These optimization mainly targets the ability of the different searches to use each others result, minimizing redundant calculations and cache misses. The state-of-the-art algorithm for running multi source breadth-first search on the CPU is called MS-BFS \cite{msbfs}. MS-BFS is implemented without the usage of atomic and locking operations, making it scalable with the number of cores. 

The algorithm works by propagating a set of BFS's that have reached the same node at the same time. To begin with, one set is created for every BFS at their respective source node. The sets only contains the BFS that starts at the respective source node. In the next iterations, all sets are propagated to the neighbors of the node they were previously on. If several sets reach a node, the sets are merged. Additionally, the nodes that the BFS's have already reached are tracked and any BFS that reach a node it has already seen are ignored. 

This means that once two breadth first searches have met, they can from now on be seen as one. This decreases the amount of propagations that have to be made compared to if they all propagated the graphs individually. In the article, the sets of BFS's are lists of bits where the indices of the bits that are set signify the id of the BFS's.

\section{Maximal matching}
Given a graph $G = (V,E)$, a matching is a subset $M \subseteq E$ such that all nodes are incident to at most one edge in M. A maximal matching is then a matching such that if another edge is added to it, it is no longer a matching. 

\section{Approximation algorithms}
Approximation algorithms are useful to calculate a non-optimal answer quickly where the optimal solution would take too long to calculate. A C-approximation means that the solution is in worst case C times the size of the optimal solution. For example, if you calculate the minimal vertex cover with a 2-approximation the solution can at most be twice as large as the optimal solution. If you want to calculate the maximal matching with a $\frac{1}{2}$-approximation the size of the solution can be be no less than half of the optimal solution.

\section{Minimum vertex cover}
Given an undirected graph $G = (V,E)$, a vertex cover is a subset $S \subseteq V$ such that $\forall e = (u,v) \in E,  u \in S \vee v \in S$. A minimum vertex cover is a set $S$ of minimum size. The problem of finding a minimum vertex cover is NP-complete \cite{Kar72}.

\subsection{Approximate minimum vertex cover}
As minimum vertex cover is NP-complete there have been studies of polynomial time approximation algorithms. The state-of-the-art algorithms give $2$-approximations. For general graphs it is hard to achieve a $(2-\epsilon)$-approximation for any $\epsilon > 0$ \cite{2-evchard}, so $2$ can be the best constant approximation factor achievable.

A simple greedy $2$-approximation algorithm maintain a maximal matching $M$ to calculate a vertex cover $V$. By the definition of maximal matching it is certain that every edge are either in the maximal matching or shares a node with one in it. For all edges, $e = (u,v) \in M$, pick both $u$ and $v$ for the vertex cover, and all edges are guaranteed to be covered. For an optimal minimum vertex cover $V_{opt}$, it holds that $\forall e = (u,v) \in M, u \in V_{opt} \vee v \in V_{opt}$. This can be proved by a simple contradiction. Assume $u,v \notin V$. This implies that $e$ is not covered by any node, hence $V$ is not a vertex cover $\blacksquare$. So by picking both nodes for all edges in the maximal matching, a $2$-approximation algorithm is achieved as at least one of the nodes must be in $V_{opt}$.

\subsection{Dynamic approximate minimum vertex cover}
There exists several different algorithms for maintaining a fully dynamic approximate vertex cover \cite{2appdynvc, 2appdynvclogn, 2eappdynvc}. The main difference between the algorithms is the trade off between the time complexity for insertions and deletions. The state-of-the-art fully dynamic approximate minimum vertex cover algorithms can achieve $2$-approximations.


\section{Graph compression}
\label{sec:Compression}

The webgraph framework\cite{webgraph} uses many different ways to compress a graph but their own method is the format BVGraph. It exploits common properties seen on the relation between websites that emerge if the links are sorted in lexicographic order and given an index based on their position in the order. A relation from page $A$ to page $B$ exists if there is a hyperlink from page $A$ to $B$. One common property is that most hyperlinks on websites are navigational, i.e. leads to a different page on the same website. So if the links are sorted in lexicographic order many nodes indices will be close to each other. This property is called \emph{locality}. Another common pattern is that, as most links are navigational, many nodes close to each other have many common neighbors, called \emph{similarity}. The navigational links often refer to pages far down in the site hierarchy and all of these pages will be right beside each other because of the long mutual address prefix. So many of the neighbors form long consecutive intervals of one-increasing indices, a property called \emph{consecutivity}. \cite{webgraph-compression}

To take advantage of the \emph{locality} property a neighbor in a graph are represented as a difference from the previous neighbor. This makes the numbers significantly smaller and can take up less space. Another compression method called copy-blocks exploits the \emph{similarity} property and copies neighbors from a node nearby. If two nodes have almost the exact same neighbors it saves a lot of space. The \emph{consecutivity} property is used by saving large portions of intervals as just two numbers: the start node and the interval length. \cite{webgraph-compression}

A way to store integers of varying size needs to be used as the compression uses methods to minimise the size of numbers. There are several methods to do this which have different advantages and disadvantages. Webgraph use different methods for different usages to optimize the space used. \cite{webgraph-compression}

\subsection{Layered Label Propagation}

While the properties mentioned above are trivially achieved in a graph of hyperlinks, they might be harder to find in different types of graphs. To be able to use the compression techniques in other types of graphs P. Boldie et al have developed a method called LLP (Layered Label Propagation) to reorder the node indices to use the compressions more efficiently. \cite{llp} 

